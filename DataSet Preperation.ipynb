{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0120e82e-86b0-45df-93e4-af377e4e9a5d",
   "metadata": {},
   "source": [
    "# Combining both Positive and Negative .npy file creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0876e977-4bec-4522-b5f8-5d59fa4924f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Optional, Union, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from rasterio.windows import bounds as window_bounds\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from rasterio.mask import mask\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_reference_grid(\n",
    "    ca_boundary: str,\n",
    "    out_path: str,\n",
    "    dst_crs: str = \"EPSG:3310\",\n",
    "    dst_res: float = 100.0,\n",
    "    compress: str = \"lzw\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create a reference grid that covers California at specified resolution.\n",
    "    This grid will be used to align ALL rasters (static and dynamic).\n",
    "    \n",
    "    Returns: path to the reference grid raster\n",
    "    \"\"\"\n",
    "    print(f\"[Creating Reference Grid]\")\n",
    "    ca = gpd.read_file(ca_boundary)\n",
    "    if ca.crs is None:\n",
    "        raise ValueError(\"CA boundary has no CRS\")\n",
    "    \n",
    "    ca = ca.to_crs(dst_crs)\n",
    "    bounds = ca.total_bounds  # minx, miny, maxx, maxy\n",
    "    \n",
    "    # Calculate dimensions\n",
    "    width = int(np.ceil((bounds[2] - bounds[0]) / dst_res))\n",
    "    height = int(np.ceil((bounds[3] - bounds[1]) / dst_res))\n",
    "    \n",
    "    # Create transform\n",
    "    from rasterio.transform import from_origin\n",
    "    transform = from_origin(bounds[0], bounds[3], dst_res, dst_res)\n",
    "    \n",
    "    # Create empty reference raster\n",
    "    profile = {\n",
    "        'driver': 'GTiff',\n",
    "        'height': height,\n",
    "        'width': width,\n",
    "        'count': 1,\n",
    "        'dtype': 'float32',\n",
    "        'crs': dst_crs,\n",
    "        'transform': transform,\n",
    "        'nodata': -9999.0,\n",
    "        'compress': compress\n",
    "    }\n",
    "    \n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    with rasterio.open(out_path, 'w', **profile) as dst:\n",
    "        dst.write(np.zeros((height, width), dtype='float32'), 1)\n",
    "    \n",
    "    print(f\"  ✓ Reference grid created: {width}x{height} px at {dst_res}m resolution\")\n",
    "    print(f\"  ✓ Saved to: {out_path}\")\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def align_raster_to_reference(\n",
    "    src_raster: str,\n",
    "    reference_grid: str,\n",
    "    out_path: str,\n",
    "    ca_boundary: str,\n",
    "    compress: str = \"lzw\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Reproject and snap source raster to reference grid, then clip to CA boundary.\n",
    "    \"\"\"    \n",
    "    # Convert to Path object and create parent directories\n",
    "    out_path = Path(out_path)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Step 1: Reproject to match reference grid\n",
    "    with rasterio.open(reference_grid) as ref, rasterio.open(src_raster) as src:\n",
    "        profile = ref.profile.copy()\n",
    "        profile.update({'compress': compress})\n",
    "        \n",
    "        temp_path = str(out_path.parent / (out_path.name + \".temp.tif\"))\n",
    "        with rasterio.open(temp_path, 'w', **profile) as dst:\n",
    "            reproject(\n",
    "                source=rasterio.band(src, 1),\n",
    "                destination=rasterio.band(dst, 1),\n",
    "                src_transform=src.transform,\n",
    "                src_crs=src.crs,\n",
    "                dst_transform=ref.transform,\n",
    "                dst_crs=ref.crs,\n",
    "                resampling=Resampling.nearest,  # Use nearest for categorical data\n",
    "                src_nodata=src.nodata,\n",
    "                dst_nodata=ref.nodata\n",
    "            )\n",
    "    \n",
    "    # Step 2: Clip to CA boundary\n",
    "    ca = gpd.read_file(ca_boundary)\n",
    "    with rasterio.open(reference_grid) as ref:\n",
    "        if ca.crs != ref.crs:\n",
    "            ca = ca.to_crs(ref.crs)\n",
    "    \n",
    "    geoms = [geom.__geo_interface__ for geom in ca.geometry if geom is not None]\n",
    "    \n",
    "    with rasterio.open(temp_path) as src:\n",
    "        out_image, out_transform = mask(src, geoms, crop=True, nodata=src.nodata, filled=True)\n",
    "        out_meta = src.meta.copy()\n",
    "        out_meta.update({\n",
    "            \"height\": out_image.shape[1],\n",
    "            \"width\": out_image.shape[2],\n",
    "            \"transform\": out_transform,\n",
    "            \"compress\": compress\n",
    "        })\n",
    "        \n",
    "        with rasterio.open(str(out_path), \"w\", **out_meta) as dst:\n",
    "            dst.write(out_image)\n",
    "    \n",
    "    # Clean up temp file\n",
    "    if os.path.exists(temp_path):\n",
    "        os.remove(temp_path)\n",
    "    \n",
    "    return str(out_path)\n",
    "\n",
    "\n",
    "def discover_dynamic_rasters(\n",
    "    dynamic_folders: Dict[str, str],\n",
    "    pattern: str = \"*.tif\"\n",
    ") -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Discover all .tif files in each dynamic variable folder.\n",
    "    \n",
    "    Args:\n",
    "        dynamic_folders: {variable_name: folder_path containing all tifs for that variable}\n",
    "        pattern: file pattern to match (default: \"*.tif\")\n",
    "    \n",
    "    Returns:\n",
    "        {variable_name: [sorted list of paths]}\n",
    "    \"\"\"\n",
    "    dynamic_rasters = {}\n",
    "    for var_name, folder_path in dynamic_folders.items():\n",
    "        folder = Path(folder_path)\n",
    "        if not folder.exists():\n",
    "            raise ValueError(f\"Folder not found: {folder_path}\")\n",
    "        \n",
    "        tif_files = sorted(folder.glob(pattern))\n",
    "        if not tif_files:\n",
    "            raise ValueError(f\"No .tif files found in {folder_path}\")\n",
    "        \n",
    "        dynamic_rasters[var_name] = [str(p) for p in tif_files]\n",
    "        print(f\"  {var_name}: found {len(tif_files)} files\")\n",
    "    \n",
    "    return dynamic_rasters\n",
    "\n",
    "\n",
    "def align_all_rasters(\n",
    "    static_rasters: Dict[str, str],  # {variable_name: path}\n",
    "    dynamic_rasters: Dict[str, List[str]],  # {variable_name: [path1, path2, ...]}\n",
    "    landcover_raster: Optional[str],  # NEW: landcover path\n",
    "    reference_grid: str,\n",
    "    ca_boundary: str,\n",
    "    out_root: str,\n",
    "    compress: str = \"lzw\",\n",
    "    skip_existing: bool = True,  # Skip if aligned file already exists\n",
    "    stats_json_path: Optional[str] = None  # Path to save statistics JSON\n",
    ") -> Tuple[Dict[str, str], Dict[str, List[str]], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Align all static, dynamic, and landcover rasters to the reference grid.\n",
    "    Also computes and saves min/max statistics for each variable.\n",
    "    \n",
    "    Args:\n",
    "        landcover_raster: Path to landcover.tif (optional)\n",
    "        skip_existing: If True, skip alignment if output file already exists\n",
    "        stats_json_path: Path to save statistics JSON (default: out_root/statistics.json)\n",
    "    \n",
    "    Returns:\n",
    "        (aligned_static_paths, aligned_dynamic_paths, aligned_landcover_path)\n",
    "    \"\"\"\n",
    "    # Initialize statistics dictionary\n",
    "    statistics = {\n",
    "        \"static\": {\"min\": {}, \"max\": {}},\n",
    "        \"dynamic\": {\"min\": {}, \"max\": {}}\n",
    "    }\n",
    "    \n",
    "    # Set default stats path if not provided\n",
    "    if stats_json_path is None:\n",
    "        stats_json_path = os.path.join(out_root, \"statistics.json\")\n",
    "    \n",
    "    print(\"\\n[Aligning Static Rasters]\")\n",
    "    aligned_static = {}\n",
    "    for var_name, src_path in static_rasters.items():\n",
    "        out_path = os.path.join(out_root, \"aligned_static\", f\"{var_name}_aligned.tif\")\n",
    "        \n",
    "        if skip_existing and os.path.exists(out_path):\n",
    "            print(f\"  ✓ Using existing: {Path(out_path).name}\")\n",
    "            aligned_static[var_name] = out_path\n",
    "        else:\n",
    "            aligned_static[var_name] = align_raster_to_reference(\n",
    "                src_path, reference_grid, out_path, ca_boundary, compress\n",
    "            )\n",
    "        \n",
    "        # Compute statistics for this static variable\n",
    "        print(f\"  Computing statistics for {var_name}...\")\n",
    "        with rasterio.open(aligned_static[var_name]) as src:\n",
    "            arr = src.read(1)\n",
    "            nodata_val = src.nodata\n",
    "            \n",
    "            # Create valid mask\n",
    "            if nodata_val is not None:\n",
    "                valid_mask = (arr != nodata_val) & ~np.isnan(arr)\n",
    "            else:\n",
    "                valid_mask = ~np.isnan(arr)\n",
    "            \n",
    "            if np.any(valid_mask):\n",
    "                valid_data = arr[valid_mask].astype(float)\n",
    "                statistics[\"static\"][\"min\"][var_name] = float(np.nanmin(valid_data))\n",
    "                statistics[\"static\"][\"max\"][var_name] = float(np.nanmax(valid_data))\n",
    "                print(f\"    Min: {statistics['static']['min'][var_name]:.4f}, \"\n",
    "                      f\"Max: {statistics['static']['max'][var_name]:.4f}\")\n",
    "    \n",
    "    # NEW: Align landcover\n",
    "    aligned_landcover = None\n",
    "    if landcover_raster and os.path.exists(landcover_raster):\n",
    "        print(\"\\n[Aligning Landcover Raster]\")\n",
    "        out_path = os.path.join(out_root, \"aligned_static\", \"landcover_aligned.tif\")\n",
    "        \n",
    "        if skip_existing and os.path.exists(out_path):\n",
    "            print(f\"  ✓ Using existing: {Path(out_path).name}\")\n",
    "            aligned_landcover = out_path\n",
    "        else:\n",
    "            aligned_landcover = align_raster_to_reference(\n",
    "                landcover_raster, reference_grid, out_path, ca_boundary, compress\n",
    "            )\n",
    "    \n",
    "    print(\"\\n[Aligning Dynamic Rasters]\")\n",
    "    aligned_dynamic = {}\n",
    "    \n",
    "    # Initialize min/max tracking for dynamic variables\n",
    "    dynamic_stats = {}\n",
    "    for var_name in dynamic_rasters.keys():\n",
    "        dynamic_stats[var_name] = {\"min\": np.inf, \"max\": -np.inf}\n",
    "    \n",
    "    for var_name, src_paths in dynamic_rasters.items():\n",
    "        print(f\"\\n  Processing {var_name}: {len(src_paths)} files\")\n",
    "        aligned_dynamic[var_name] = []\n",
    "        \n",
    "        for src_path in tqdm(src_paths, desc=f\"  Aligning {var_name}\"):\n",
    "            # Extract date from filename\n",
    "            date_match = re.search(r'(\\d{8})', Path(src_path).stem)\n",
    "            date_str = date_match.group(1) if date_match else f\"t{len(aligned_dynamic[var_name]):06d}\"\n",
    "            \n",
    "            out_path = os.path.join(\n",
    "                out_root, \"aligned_dynamic\", var_name, f\"{var_name}_{date_str}_aligned.tif\"\n",
    "            )\n",
    "            \n",
    "            if skip_existing and os.path.exists(out_path):\n",
    "                aligned_dynamic[var_name].append(out_path)\n",
    "            else:\n",
    "                aligned_dynamic[var_name].append(\n",
    "                    align_raster_to_reference(src_path, reference_grid, out_path, ca_boundary, compress)\n",
    "                )\n",
    "            \n",
    "            # Update statistics from this raster\n",
    "            with rasterio.open(aligned_dynamic[var_name][-1]) as src:\n",
    "                arr = src.read(1)\n",
    "                nodata_val = src.nodata\n",
    "                \n",
    "                # Create valid mask\n",
    "                if nodata_val is not None:\n",
    "                    valid_mask = (arr != nodata_val) & ~np.isnan(arr)\n",
    "                else:\n",
    "                    valid_mask = ~np.isnan(arr)\n",
    "                \n",
    "                if np.any(valid_mask):\n",
    "                    valid_data = arr[valid_mask].astype(float)\n",
    "                    file_min = float(np.nanmin(valid_data))\n",
    "                    file_max = float(np.nanmax(valid_data))\n",
    "                    \n",
    "                    # Update running min/max\n",
    "                    dynamic_stats[var_name][\"min\"] = min(dynamic_stats[var_name][\"min\"], file_min)\n",
    "                    dynamic_stats[var_name][\"max\"] = max(dynamic_stats[var_name][\"max\"], file_max)\n",
    "        \n",
    "        # Store final statistics for this variable\n",
    "        statistics[\"dynamic\"][\"min\"][var_name] = dynamic_stats[var_name][\"min\"]\n",
    "        statistics[\"dynamic\"][\"max\"][var_name] = dynamic_stats[var_name][\"max\"]\n",
    "        print(f\"  Statistics for {var_name}:\")\n",
    "        print(f\"    Min: {statistics['dynamic']['min'][var_name]:.4f}, \"\n",
    "              f\"Max: {statistics['dynamic']['max'][var_name]:.4f}\")\n",
    "    \n",
    "    # Save statistics to JSON\n",
    "    print(f\"\\n[Saving Statistics]\")\n",
    "    os.makedirs(os.path.dirname(stats_json_path) or \".\", exist_ok=True)\n",
    "    with open(stats_json_path, 'w') as f:\n",
    "        json.dump(statistics, f, indent=2)\n",
    "    print(f\"  ✓ Statistics saved to: {stats_json_path}\")\n",
    "    \n",
    "    return aligned_static, aligned_dynamic, aligned_landcover\n",
    "\n",
    "\n",
    "DATE8_RE = re.compile(r\"(?<!\\d)(\\d{8})(?!\\d)\")\n",
    "\n",
    "def parse_date_from_path(raster_path: str) -> Optional[datetime]:\n",
    "    \"\"\"Extract date from filename (YYYYMMDD format)\"\"\"\n",
    "    m = DATE8_RE.search(Path(raster_path).stem)\n",
    "    if m:\n",
    "        try:\n",
    "            return datetime.strptime(m.group(1), \"%Y%m%d\")\n",
    "        except:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def one_hot_encode_landcover(landcover_patch: np.ndarray, num_classes: int = 10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert landcover patch to one-hot encoded format.\n",
    "    \n",
    "    Args:\n",
    "        landcover_patch: 2D array with landcover classes (1-10)\n",
    "        num_classes: Number of landcover classes (default: 10)\n",
    "    \n",
    "    Returns:\n",
    "        3D array of shape (num_classes, height, width) with binary values\n",
    "    \"\"\"\n",
    "    height, width = landcover_patch.shape\n",
    "    one_hot = np.zeros((num_classes, height, width), dtype=np.float32)\n",
    "    \n",
    "    valid_mask = ~np.isnan(landcover_patch)\n",
    "    \n",
    "    for class_id in range(1, num_classes + 1):\n",
    "        class_mask = (landcover_patch == class_id) & valid_mask\n",
    "        one_hot[class_id - 1] = class_mask.astype(np.float32)\n",
    "    \n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def build_aligned_fire_dataset(\n",
    "    aligned_static: Dict[str, str],  \n",
    "    aligned_dynamic: Dict[str, List[str]], \n",
    "    aligned_landcover: Optional[str],  \n",
    "    fire_dataset: str,\n",
    "    fire_date_field: str,\n",
    "    ca_boundary: str,\n",
    "    out_root: str = \"output_aligned_fire_patches\",\n",
    "    tile_size: int = 25,\n",
    "    window_days: int = 10,\n",
    "    min_pos_frac: float = 0.80,\n",
    "    min_valid_frac: float = 0.50,\n",
    "    require_all_valid: bool = True,\n",
    "    strict_inside: bool = True,\n",
    "    num_landcover_classes: int = 10,  # NEW: number of landcover classes\n",
    "    # Controls for individual TIFs\n",
    "    write_individual_static_tifs: bool = True,\n",
    "    write_individual_dynamic_tifs_days: int = 2,\n",
    "    write_individual_landcover_tifs: bool = True,  # NEW\n",
    "    tif_compress: str = \"lzw\",\n",
    "    tif_nodata: float = -9999.0,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create aligned .npy patches for static, dynamic, and landcover data.\n",
    "    \n",
    "    NEW: Landcover is one-hot encoded to shape (num_classes, tile_size, tile_size)\n",
    "    \n",
    "    Output files:\n",
    "      • Static NPY:     YYYYMMDD_row_col_static.npy\n",
    "      • Dynamic NPY:    YYYYMMDD_row_col_dynamic.npy\n",
    "      • Landcover NPY:  YYYYMMDD_row_col_clc_vec.npy  (one-hot encoded)\n",
    "    \"\"\"\n",
    "    from rasterio.windows import transform as window_transform\n",
    "\n",
    "    out_root = Path(out_root)\n",
    "    out_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Reference from first static raster\n",
    "    ref_path = next(iter(aligned_static.values()))\n",
    "    with rasterio.open(ref_path) as ref:\n",
    "        ref_crs = ref.crs\n",
    "        ref_transform = ref.transform\n",
    "        ref_height = ref.height\n",
    "        ref_width = ref.width\n",
    "\n",
    "    print(f\"\\n[Reference Grid] {ref_width}x{ref_height} px\")\n",
    "\n",
    "    # Boundary\n",
    "    ca = gpd.read_file(ca_boundary)\n",
    "    if ca.crs != ref_crs:\n",
    "        ca = ca.to_crs(ref_crs)\n",
    "    ca_union = ca.unary_union\n",
    "\n",
    "    # Fires\n",
    "    print(\"\\n[Loading Fire Dataset]\")\n",
    "    fires = gpd.read_file(fire_dataset)\n",
    "    if fires.crs != ref_crs:\n",
    "        fires = fires.to_crs(ref_crs)\n",
    "    if fire_date_field not in fires.columns:\n",
    "        raise ValueError(f\"Fire date field '{fire_date_field}' not found\")\n",
    "\n",
    "    def _pfd(v):\n",
    "        if pd.isna(v): return None\n",
    "        try: return pd.to_datetime(str(v)).date()\n",
    "        except: return None\n",
    "\n",
    "    fires['__fire_date__'] = fires[fire_date_field].apply(_pfd)\n",
    "    fires = fires.dropna(subset=['__fire_date__'])\n",
    "    fires_by_date = {d: grp for d, grp in fires.groupby('__fire_date__')}\n",
    "    print(f\"  ✓ Found {len(fires)} fire polygons across {len(fires_by_date)} unique dates\")\n",
    "\n",
    "    # Dynamic index\n",
    "    print(\"\\n[Indexing Dynamic Rasters]\")\n",
    "    dynamic_index = {}\n",
    "    for var_name, paths in aligned_dynamic.items():\n",
    "        dynamic_index[var_name] = {}\n",
    "        for p in paths:\n",
    "            dt = parse_date_from_path(p)\n",
    "            if dt:\n",
    "                dynamic_index[var_name][dt.date()] = p\n",
    "        print(f\"  {var_name}: {len(dynamic_index[var_name])} dates\")\n",
    "\n",
    "    # Date intersection across dynamic vars\n",
    "    all_dates = set.intersection(*[set(d.keys()) for d in dynamic_index.values()])\n",
    "    all_dates = sorted(all_dates)\n",
    "    print(f\"  ✓ {len(all_dates)} dates available across all dynamic variables\")\n",
    "\n",
    "    # Output dirs\n",
    "    # static_dir = out_root / \"static_patches\"\n",
    "    # dynamic_dir = out_root / \"dynamic_patches\"\n",
    "    # landcover_dir = out_root / \"landcover_patches\"  # NEW\n",
    "    # static_dir.mkdir(exist_ok=True)\n",
    "    # dynamic_dir.mkdir(exist_ok=True)\n",
    "    # landcover_dir.mkdir(exist_ok=True)\n",
    "    npy_dir = Path(out_root) / \"positive\"\n",
    "    npy_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Individual per-variable GeoTIFF dirs\n",
    "    static_tif_root = out_root / \"static_tifs\"\n",
    "    dynamic_tif_root = out_root / \"dynamic_tifs\"\n",
    "    landcover_tif_root = out_root / \"landcover_tifs\"  # NEW\n",
    "    \n",
    "    if write_individual_static_tifs:\n",
    "        static_tif_root.mkdir(exist_ok=True)\n",
    "    if write_individual_dynamic_tifs_days and write_individual_dynamic_tifs_days > 0:\n",
    "        dynamic_tif_root.mkdir(exist_ok=True)\n",
    "    if write_individual_landcover_tifs and aligned_landcover:\n",
    "        landcover_tif_root.mkdir(exist_ok=True)\n",
    "\n",
    "    manifest_rows = []\n",
    "    patch_id = 0\n",
    "\n",
    "    static_vars = list(aligned_static.keys())\n",
    "    dynamic_vars = list(aligned_dynamic.keys())\n",
    "\n",
    "    print(f\"\\n[Extracting Patches]\")\n",
    "    print(f\"  Static variables: {static_vars}\")\n",
    "    print(f\"  Dynamic variables: {dynamic_vars}\")\n",
    "    print(f\"  Landcover classes: {num_landcover_classes if aligned_landcover else 'N/A'}\")\n",
    "    print(f\"  Window days: {window_days}\")\n",
    "\n",
    "    for T in tqdm(all_dates, desc=\"Processing dates\"):\n",
    "        fires_T = fires_by_date.get(T)\n",
    "        if fires_T is None or fires_T.empty:\n",
    "            continue\n",
    "\n",
    "        sindex = fires_T.sindex\n",
    "\n",
    "        needed_dates = [T - timedelta(days=i) for i in range(window_days)]\n",
    "        if not all(d in all_dates for d in needed_dates):\n",
    "            continue\n",
    "\n",
    "        rows = range(0, ref_height - (ref_height % tile_size), tile_size)\n",
    "        cols = range(0, ref_width - (ref_width % tile_size), tile_size)\n",
    "\n",
    "        for r0 in rows:\n",
    "            for c0 in cols:\n",
    "                win = Window(col_off=c0, row_off=r0, width=tile_size, height=tile_size)\n",
    "                left, bottom, right, top = window_bounds(win, transform=ref_transform)\n",
    "                tile_poly = box(left, bottom, right, top)\n",
    "\n",
    "                if strict_inside and not ca_union.covers(tile_poly):\n",
    "                    continue\n",
    "\n",
    "                cand_idx = list(sindex.intersection(tile_poly.bounds))\n",
    "                if not cand_idx:\n",
    "                    continue\n",
    "\n",
    "                inter_area = 0.0\n",
    "                for geom in fires_T.geometry.iloc[cand_idx]:\n",
    "                    if geom is None or geom.is_empty:\n",
    "                        continue\n",
    "                    try:\n",
    "                        inter_area += geom.intersection(tile_poly).area\n",
    "                    except Exception:\n",
    "                        inter_area += geom.buffer(0).intersection(tile_poly).area\n",
    "\n",
    "                tile_area = tile_poly.area\n",
    "                overlap_frac = inter_area / tile_area if tile_area > 0 else 0.0\n",
    "                if overlap_frac < min_pos_frac:\n",
    "                    continue\n",
    "\n",
    "                # --- Static stack ---\n",
    "                static_stack = []\n",
    "                valid_static = True\n",
    "                for var_name in static_vars:\n",
    "                    with rasterio.open(aligned_static[var_name]) as src:\n",
    "                        patch = src.read(1, window=win, masked=True)\n",
    "                        if patch.shape != (tile_size, tile_size):\n",
    "                            valid_static = False; break\n",
    "                        invalid = patch.mask\n",
    "                        if require_all_valid and invalid.any():\n",
    "                            valid_static = False; break\n",
    "                        elif not require_all_valid:\n",
    "                            valid_frac = 1.0 - float(invalid.mean())\n",
    "                            if valid_frac < min_valid_frac:\n",
    "                                valid_static = False; break\n",
    "                        arr = np.asarray(patch, dtype=np.float32)\n",
    "                        if invalid.any():\n",
    "                            arr = np.where(invalid, np.nan, arr)\n",
    "                        static_stack.append(arr)\n",
    "                if not valid_static:\n",
    "                    continue\n",
    "\n",
    "                # --- Landcover (one-hot encoded) ---\n",
    "                landcover_array = None\n",
    "                if aligned_landcover:\n",
    "                    with rasterio.open(aligned_landcover) as src:\n",
    "                        lc_patch = src.read(1, window=win, masked=True)\n",
    "                        if lc_patch.shape != (tile_size, tile_size):\n",
    "                            continue\n",
    "                        \n",
    "                        # Convert to float and handle nodata\n",
    "                        lc_arr = np.asarray(lc_patch, dtype=np.float32)\n",
    "                        if lc_patch.mask.any():\n",
    "                            lc_arr = np.where(lc_patch.mask, np.nan, lc_arr)\n",
    "                        \n",
    "                        # One-hot encode\n",
    "                        landcover_array = one_hot_encode_landcover(lc_arr, num_landcover_classes)\n",
    "\n",
    "                # --- Dynamic stack ---\n",
    "                dynamic_stack = []\n",
    "                valid_dynamic = True\n",
    "                for date_offset in range(window_days):\n",
    "                    target_date = T - timedelta(days=date_offset)\n",
    "                    day_stack = []\n",
    "                    for var_name in dynamic_vars:\n",
    "                        raster_path = dynamic_index[var_name][target_date]\n",
    "                        with rasterio.open(raster_path) as src:\n",
    "                            patch = src.read(1, window=win, masked=True)\n",
    "                            if patch.shape != (tile_size, tile_size):\n",
    "                                valid_dynamic = False; break\n",
    "                            invalid = patch.mask\n",
    "                            if require_all_valid and invalid.any():\n",
    "                                valid_dynamic = False; break\n",
    "                            elif not require_all_valid:\n",
    "                                valid_frac = 1.0 - float(invalid.mean())\n",
    "                                if valid_frac < min_valid_frac:\n",
    "                                    valid_dynamic = False; break\n",
    "                            arr = np.asarray(patch, dtype=np.float32)\n",
    "                            if invalid.any():\n",
    "                                arr = np.where(invalid, np.nan, arr)\n",
    "                            day_stack.append(arr)\n",
    "                    if not valid_dynamic:\n",
    "                        break\n",
    "                    dynamic_stack.append(np.stack(day_stack, axis=0))\n",
    "                if not valid_dynamic:\n",
    "                    continue\n",
    "\n",
    "                # --- Save NPYs ---\n",
    "                patch_id += 1\n",
    "                fire_day_str = T.strftime(\"%Y%m%d\")\n",
    "                static_array = np.stack(static_stack, axis=0)\n",
    "                dynamic_array = np.stack(dynamic_stack, axis=0)\n",
    "\n",
    "                static_path = npy_dir / f\"{fire_day_str}_{int(r0)}_{int(c0)}_static.npy\"\n",
    "                dynamic_path = npy_dir / f\"{fire_day_str}_{int(r0)}_{int(c0)}_dynamic.npy\"\n",
    "                np.save(static_path, static_array.astype(np.float32))\n",
    "                np.save(dynamic_path, dynamic_array.astype(np.float32))\n",
    "                \n",
    "                # NEW: Save landcover as clc_vec.npy\n",
    "                landcover_path = None\n",
    "                if landcover_array is not None:\n",
    "                    landcover_path = npy_dir / f\"{fire_day_str}_{int(r0)}_{int(c0)}_clc_vec.npy\"\n",
    "                    np.save(landcover_path, landcover_array.astype(np.float32))\n",
    "\n",
    "                # --- Write individual GeoTIFFs ---\n",
    "                tile_transform = window_transform(win, ref_transform)\n",
    "\n",
    "                # 1) Static TIFs\n",
    "                if write_individual_static_tifs:\n",
    "                    for idx, var_name in enumerate(static_vars):\n",
    "                        arr = static_array[idx]\n",
    "                        var_dir = (static_tif_root / var_name)\n",
    "                        var_dir.mkdir(parents=True, exist_ok=True)\n",
    "                        out_tif = var_dir / f\"{fire_day_str}_{int(r0)}_{int(c0)}_static.tif\"\n",
    "                        profile = {\n",
    "                            \"driver\": \"GTiff\",\n",
    "                            \"height\": tile_size,\n",
    "                            \"width\": tile_size,\n",
    "                            \"count\": 1,\n",
    "                            \"dtype\": \"float32\",\n",
    "                            \"crs\": ref_crs,\n",
    "                            \"transform\": tile_transform,\n",
    "                            \"nodata\": tif_nodata,\n",
    "                            \"compress\": tif_compress,\n",
    "                        }\n",
    "                        with rasterio.open(out_tif, \"w\", **profile) as dst:\n",
    "                            dst.write(np.where(np.isnan(arr), tif_nodata, arr).astype(np.float32), 1)\n",
    "                            dst.update_tags(\n",
    "                                patch_id=int(patch_id),\n",
    "                                fire_date=str(T),\n",
    "                                row_off=int(r0),\n",
    "                                col_off=int(c0),\n",
    "                                var_name=var_name,\n",
    "                            )\n",
    "\n",
    "                # 2) Dynamic TIFs (first K days)\n",
    "                K = max(0, min(int(write_individual_dynamic_tifs_days), window_days))\n",
    "                for d in range(K):\n",
    "                    day_date = (T - timedelta(days=d)).strftime(\"%Y%m%d\")\n",
    "                    for vidx, var_name in enumerate(dynamic_vars):\n",
    "                        arr = dynamic_array[d, vidx]\n",
    "                        var_dir = (dynamic_tif_root / var_name)\n",
    "                        var_dir.mkdir(parents=True, exist_ok=True)\n",
    "                        out_tif = var_dir / f\"{day_date}_{int(r0)}_{int(c0)}_dynamic.tif\"\n",
    "                        profile = {\n",
    "                            \"driver\": \"GTiff\",\n",
    "                            \"height\": tile_size,\n",
    "                            \"width\": tile_size,\n",
    "                            \"count\": 1,\n",
    "                            \"dtype\": \"float32\",\n",
    "                            \"crs\": ref_crs,\n",
    "                            \"transform\": tile_transform,\n",
    "                            \"nodata\": tif_nodata,\n",
    "                            \"compress\": tif_compress,\n",
    "                        }\n",
    "                        with rasterio.open(out_tif, \"w\", **profile) as dst:\n",
    "                            dst.write(np.where(np.isnan(arr), tif_nodata, arr).astype(np.float32), 1)\n",
    "                            dst.update_tags(\n",
    "                                patch_id=int(patch_id),\n",
    "                                fire_date=str(T),\n",
    "                                day_index=int(d),\n",
    "                                day_date=day_date,\n",
    "                                row_off=int(r0),\n",
    "                                col_off=int(c0),\n",
    "                                var_name=var_name,\n",
    "                            )\n",
    "\n",
    "                # 3) NEW: Landcover TIFs (one per class)\n",
    "                if write_individual_landcover_tifs and landcover_array is not None:\n",
    "                    for class_id in range(num_landcover_classes):\n",
    "                        arr = landcover_array[class_id]\n",
    "                        class_name = f\"class_{class_id + 1}\"\n",
    "                        var_dir = (landcover_tif_root / class_name)\n",
    "                        var_dir.mkdir(parents=True, exist_ok=True)\n",
    "                        out_tif = var_dir / f\"{fire_day_str}_{int(r0)}_{int(c0)}_clc_vec.tif\"\n",
    "                        profile = {\n",
    "                            \"driver\": \"GTiff\",\n",
    "                            \"height\": tile_size,\n",
    "                            \"width\": tile_size,\n",
    "                            \"count\": 1,\n",
    "                            \"dtype\": \"float32\",\n",
    "                            \"crs\": ref_crs,\n",
    "                            \"transform\": tile_transform,\n",
    "                            \"nodata\": tif_nodata,\n",
    "                            \"compress\": tif_compress,\n",
    "                        }\n",
    "                        with rasterio.open(out_tif, \"w\", **profile) as dst:\n",
    "                            dst.write(arr.astype(np.float32), 1)\n",
    "                            dst.update_tags(\n",
    "                                patch_id=int(patch_id),\n",
    "                                fire_date=str(T),\n",
    "                                row_off=int(r0),\n",
    "                                col_off=int(c0),\n",
    "                                landcover_class=int(class_id + 1),\n",
    "                            )\n",
    "\n",
    "                # Manifest\n",
    "                manifest_row = {\n",
    "                    'patch_id': patch_id,\n",
    "                    'static_path': str(static_path),\n",
    "                    'dynamic_path': str(dynamic_path),\n",
    "                    'fire_date': T.strftime('%Y-%m-%d'),\n",
    "                    'row_off': r0,\n",
    "                    'col_off': c0,\n",
    "                    'left': left,\n",
    "                    'bottom': bottom,\n",
    "                    'right': right,\n",
    "                    'top': top,\n",
    "                    'overlap_frac': overlap_frac,\n",
    "                    'static_vars': ','.join(static_vars),\n",
    "                    'dynamic_vars': ','.join(dynamic_vars),\n",
    "                    'window_days': window_days,\n",
    "                }\n",
    "                if landcover_path:\n",
    "                    manifest_row['landcover_path'] = str(landcover_path)\n",
    "                    manifest_row['landcover_classes'] = num_landcover_classes\n",
    "                \n",
    "                manifest_rows.append(manifest_row)\n",
    "\n",
    "    # Save manifest\n",
    "    manifest = pd.DataFrame(manifest_rows)\n",
    "    manifest_path = out_root / \"aligned_patches_manifest.csv\"\n",
    "    manifest.to_csv(manifest_path, index=False)\n",
    "\n",
    "    print(f\"\\n✅ Complete!\")\n",
    "    print(f\"  Total patches: {len(manifest)}\")\n",
    "    print(f\"  Static shape: ({len(static_vars)}, {tile_size}, {tile_size})\")\n",
    "    print(f\"  Dynamic shape: ({window_days}, {len(dynamic_vars)}, {tile_size}, {tile_size})\")\n",
    "    if aligned_landcover:\n",
    "        print(f\"  Landcover shape: ({num_landcover_classes}, {tile_size}, {tile_size}) [one-hot encoded]\")\n",
    "    print(f\"  Manifest: {manifest_path}\")\n",
    "    print(f\"  Static TIFs: {static_tif_root if write_individual_static_tifs else 'disabled'}\")\n",
    "    print(f\"  Dynamic TIFs (first {write_individual_dynamic_tifs_days} days): \"\n",
    "          f\"{dynamic_tif_root if write_individual_dynamic_tifs_days>0 else 'disabled'}\")\n",
    "    if aligned_landcover:\n",
    "        print(f\"  Landcover TIFs: {landcover_tif_root if write_individual_landcover_tifs else 'disabled'}\")\n",
    "\n",
    "    return manifest\n",
    "\n",
    "\n",
    "def build_negative_fire_dataset(\n",
    "    aligned_static: Dict[str, str],\n",
    "    aligned_dynamic: Dict[str, List[str]],\n",
    "    aligned_landcover: Optional[str],\n",
    "    fire_dataset: str,\n",
    "    fire_date_field: str,\n",
    "    ca_boundary: str,\n",
    "    out_root: str = \"output_aligned_fire_patches\",\n",
    "    tile_size: int = 25,\n",
    "    window_days: int = 10,\n",
    "    samples_per_year: int = 1000,\n",
    "    min_valid_frac: float = 1,\n",
    "    require_all_valid: bool = True,\n",
    "    num_landcover_classes: int = 10,\n",
    "    start_year: int = 2015,\n",
    "    end_year: int = 2024,\n",
    "    max_attempts_per_year: int = 50000,\n",
    "    tif_nodata: float = -9999.0,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create negative samples where NO fire occurred in the 10-day window at sampled locations.\n",
    "    \n",
    "    Args:\n",
    "        samples_per_year: Number of negative samples to generate per year (default: 3000)\n",
    "        start_year: First year to sample from (default: 2015)\n",
    "        end_year: Last year to sample from (default: 2024)\n",
    "        max_attempts_per_year: Maximum random sampling attempts per year before giving up\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with manifest of negative samples\n",
    "    \"\"\"\n",
    "    from rasterio.windows import transform as window_transform\n",
    "    import random\n",
    "\n",
    "    out_root = Path(out_root)\n",
    "    \n",
    "    # Reference from first static raster\n",
    "    ref_path = next(iter(aligned_static.values()))\n",
    "    with rasterio.open(ref_path) as ref:\n",
    "        ref_crs = ref.crs\n",
    "        ref_transform = ref.transform\n",
    "        ref_height = ref.height\n",
    "        ref_width = ref.width\n",
    "\n",
    "    print(f\"\\n[Reference Grid] {ref_width}x{ref_height} px\")\n",
    "\n",
    "    # Boundary\n",
    "    ca = gpd.read_file(ca_boundary)\n",
    "    if ca.crs != ref_crs:\n",
    "        ca = ca.to_crs(ref_crs)\n",
    "    ca_union = ca.unary_union\n",
    "\n",
    "    # Fires\n",
    "    print(\"\\n[Loading Fire Dataset]\")\n",
    "    fires = gpd.read_file(fire_dataset)\n",
    "    if fires.crs != ref_crs:\n",
    "        fires = fires.to_crs(ref_crs)\n",
    "    if fire_date_field not in fires.columns:\n",
    "        raise ValueError(f\"Fire date field '{fire_date_field}' not found\")\n",
    "\n",
    "    def _pfd(v):\n",
    "        if pd.isna(v): return None\n",
    "        try: return pd.to_datetime(str(v)).date()\n",
    "        except: return None\n",
    "\n",
    "    fires['__fire_date__'] = fires[fire_date_field].apply(_pfd)\n",
    "    fires = fires.dropna(subset=['__fire_date__'])\n",
    "    fires_by_date = {d: grp for d, grp in fires.groupby('__fire_date__')}\n",
    "    print(f\"  ✓ Found {len(fires)} fire polygons across {len(fires_by_date)} unique dates\")\n",
    "\n",
    "    # Dynamic index\n",
    "    print(\"\\n[Indexing Dynamic Rasters]\")\n",
    "    dynamic_index = {}\n",
    "    for var_name, paths in aligned_dynamic.items():\n",
    "        dynamic_index[var_name] = {}\n",
    "        for p in paths:\n",
    "            dt = parse_date_from_path(p)\n",
    "            if dt:\n",
    "                dynamic_index[var_name][dt.date()] = p\n",
    "        print(f\"  {var_name}: {len(dynamic_index[var_name])} dates\")\n",
    "\n",
    "    # Date intersection across dynamic vars\n",
    "    all_dates = set.intersection(*[set(d.keys()) for d in dynamic_index.values()])\n",
    "    all_dates = sorted(all_dates)\n",
    "    print(f\"  ✓ {len(all_dates)} dates available across all dynamic variables\")\n",
    "\n",
    "    negative_root = out_root / \"negative\"\n",
    "    negative_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    manifest_rows = []\n",
    "    patch_id = 0\n",
    "\n",
    "    static_vars = list(aligned_static.keys())\n",
    "    dynamic_vars = list(aligned_dynamic.keys())\n",
    "\n",
    "    print(f\"\\n[Extracting NEGATIVE Patches]\")\n",
    "    print(f\"  Static variables: {static_vars}\")\n",
    "    print(f\"  Dynamic variables: {dynamic_vars}\")\n",
    "    print(f\"  Landcover classes: {num_landcover_classes if aligned_landcover else 'N/A'}\")\n",
    "    print(f\"  Window days: {window_days}\")\n",
    "    print(f\"  Samples per year: {samples_per_year}\")\n",
    "    print(f\"  Year range: {start_year}-{end_year}\")\n",
    "\n",
    "    # Pre-compute valid tile positions (inside CA boundary)\n",
    "    print(\"\\n[Pre-computing valid tile positions]\")\n",
    "    max_row = ref_height - tile_size\n",
    "    max_col = ref_width - tile_size\n",
    "    \n",
    "    # Generate all possible tile positions\n",
    "    all_tile_positions = []\n",
    "    for r0 in range(0, max_row, tile_size):\n",
    "        for c0 in range(0, max_col, tile_size):\n",
    "            win = Window(col_off=c0, row_off=r0, width=tile_size, height=tile_size)\n",
    "            left, bottom, right, top = window_bounds(win, transform=ref_transform)\n",
    "            tile_poly = box(left, bottom, right, top)\n",
    "            \n",
    "            # Only keep tiles that are inside CA\n",
    "            if ca_union.covers(tile_poly):\n",
    "                all_tile_positions.append((r0, c0, tile_poly, left, bottom, right, top))\n",
    "    \n",
    "    print(f\"  ✓ Found {len(all_tile_positions)} valid tile positions inside California\")\n",
    "\n",
    "    # Process each year\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        print(f\"\\n[Processing Year {year}]\")\n",
    "        \n",
    "        # Get dates for this year\n",
    "        year_dates = [d for d in all_dates if d.year == year]\n",
    "        \n",
    "        # Filter dates that have 10-day window available\n",
    "        valid_dates = []\n",
    "        for T in year_dates:\n",
    "            needed_dates = [T - timedelta(days=i) for i in range(window_days)]\n",
    "            if all(d in all_dates for d in needed_dates):\n",
    "                valid_dates.append(T)\n",
    "        \n",
    "        print(f\"  Valid dates with 10-day window: {len(valid_dates)}\")\n",
    "        \n",
    "        if len(valid_dates) == 0:\n",
    "            print(f\"  ⚠️  No valid dates found for year {year}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        year_samples = 0\n",
    "        attempts = 0\n",
    "        \n",
    "        with tqdm(total=samples_per_year, desc=f\"  Sampling year {year}\") as pbar:\n",
    "            while year_samples < samples_per_year and attempts < max_attempts_per_year:\n",
    "                attempts += 1\n",
    "                \n",
    "                # Randomly select a date\n",
    "                T = random.choice(valid_dates)\n",
    "                \n",
    "                # Get 10-day window\n",
    "                needed_dates = [T - timedelta(days=i) for i in range(window_days)]\n",
    "                \n",
    "                # Check if any fire occurred in this 10-day window\n",
    "                fires_in_window = []\n",
    "                has_fire_in_window = False\n",
    "                for date in needed_dates:\n",
    "                    if date in fires_by_date:\n",
    "                        fires_in_window.extend(fires_by_date[date].geometry.tolist())\n",
    "                        has_fire_in_window = True\n",
    "                \n",
    "                # Create spatial index for fires in this window (if any)\n",
    "                if has_fire_in_window:\n",
    "                    fire_union = gpd.GeoSeries(fires_in_window, crs=ref_crs).union_all()\n",
    "                else:\n",
    "                    fire_union = None\n",
    "                \n",
    "                # Randomly select a tile position\n",
    "                r0, c0, tile_poly, left, bottom, right, top = random.choice(all_tile_positions)\n",
    "                \n",
    "                # Check if this tile intersects with any fire in the 10-day window\n",
    "                if fire_union is not None:\n",
    "                    try:\n",
    "                        if tile_poly.intersects(fire_union):\n",
    "                            continue  # Skip this tile, has fire\n",
    "                    except Exception:\n",
    "                        # Handle geometry errors\n",
    "                        if tile_poly.intersects(fire_union.buffer(0)):\n",
    "                            continue\n",
    "                \n",
    "                # This tile has NO fire in the 10-day window, extract data\n",
    "                win = Window(col_off=c0, row_off=r0, width=tile_size, height=tile_size)\n",
    "                \n",
    "                # --- Static stack ---\n",
    "                static_stack = []\n",
    "                valid_static = True\n",
    "                for var_name in static_vars:\n",
    "                    with rasterio.open(aligned_static[var_name]) as src:\n",
    "                        patch = src.read(1, window=win, masked=True)\n",
    "                        if patch.shape != (tile_size, tile_size):\n",
    "                            valid_static = False\n",
    "                            break\n",
    "                        invalid = patch.mask\n",
    "                        if require_all_valid and invalid.any():\n",
    "                            valid_static = False\n",
    "                            break\n",
    "                        elif not require_all_valid:\n",
    "                            valid_frac = 1.0 - float(invalid.mean())\n",
    "                            if valid_frac < min_valid_frac:\n",
    "                                valid_static = False\n",
    "                                break\n",
    "                        arr = np.asarray(patch, dtype=np.float32)\n",
    "                        if invalid.any():\n",
    "                            arr = np.where(invalid, np.nan, arr)\n",
    "                        static_stack.append(arr)\n",
    "                if not valid_static:\n",
    "                    continue\n",
    "\n",
    "                # --- Landcover (one-hot encoded) ---\n",
    "                landcover_array = None\n",
    "                if aligned_landcover:\n",
    "                    with rasterio.open(aligned_landcover) as src:\n",
    "                        lc_patch = src.read(1, window=win, masked=True)\n",
    "                        if lc_patch.shape != (tile_size, tile_size):\n",
    "                            continue\n",
    "                        \n",
    "                        lc_arr = np.asarray(lc_patch, dtype=np.float32)\n",
    "                        if lc_patch.mask.any():\n",
    "                            lc_arr = np.where(lc_patch.mask, np.nan, lc_arr)\n",
    "                        \n",
    "                        landcover_array = one_hot_encode_landcover(lc_arr, num_landcover_classes)\n",
    "\n",
    "                # --- Dynamic stack ---\n",
    "                dynamic_stack = []\n",
    "                valid_dynamic = True\n",
    "                for date_offset in range(window_days):\n",
    "                    target_date = T - timedelta(days=date_offset)\n",
    "                    day_stack = []\n",
    "                    for var_name in dynamic_vars:\n",
    "                        raster_path = dynamic_index[var_name][target_date]\n",
    "                        with rasterio.open(raster_path) as src:\n",
    "                            patch = src.read(1, window=win, masked=True)\n",
    "                            if patch.shape != (tile_size, tile_size):\n",
    "                                valid_dynamic = False\n",
    "                                break\n",
    "                            invalid = patch.mask\n",
    "                            if require_all_valid and invalid.any():\n",
    "                                valid_dynamic = False\n",
    "                                break\n",
    "                            elif not require_all_valid:\n",
    "                                valid_frac = 1.0 - float(invalid.mean())\n",
    "                                if valid_frac < min_valid_frac:\n",
    "                                    valid_dynamic = False\n",
    "                                    break\n",
    "                            arr = np.asarray(patch, dtype=np.float32)\n",
    "                            if invalid.any():\n",
    "                                arr = np.where(invalid, np.nan, arr)\n",
    "                            day_stack.append(arr)\n",
    "                    if not valid_dynamic:\n",
    "                        break\n",
    "                    dynamic_stack.append(np.stack(day_stack, axis=0))\n",
    "                if not valid_dynamic:\n",
    "                    continue\n",
    "\n",
    "                # --- Save NPYs ---\n",
    "                patch_id += 1\n",
    "                year_samples += 1\n",
    "                pbar.update(1)\n",
    "                \n",
    "                fire_day_str = T.strftime(\"%Y%m%d\")\n",
    "                static_array = np.stack(static_stack, axis=0)\n",
    "                dynamic_array = np.stack(dynamic_stack, axis=0)\n",
    "\n",
    "                static_path = negative_root / f\"{fire_day_str}_{int(r0)}_{int(c0)}_static.npy\"\n",
    "                dynamic_path = negative_root / f\"{fire_day_str}_{int(r0)}_{int(c0)}_dynamic.npy\"\n",
    "                np.save(static_path, static_array.astype(np.float32))\n",
    "                np.save(dynamic_path, dynamic_array.astype(np.float32))\n",
    "                \n",
    "                landcover_path = None\n",
    "                if landcover_array is not None:\n",
    "                    landcover_path = negative_root / f\"{fire_day_str}_{int(r0)}_{int(c0)}_clc_vec.npy\"\n",
    "                    np.save(landcover_path, landcover_array.astype(np.float32))\n",
    "\n",
    "                # Manifest\n",
    "                manifest_row = {\n",
    "                    'patch_id': patch_id,\n",
    "                    'static_path': str(static_path),\n",
    "                    'dynamic_path': str(dynamic_path),\n",
    "                    'sample_date': T.strftime('%Y-%m-%d'),\n",
    "                    'year': year,\n",
    "                    'row_off': r0,\n",
    "                    'col_off': c0,\n",
    "                    'left': left,\n",
    "                    'bottom': bottom,\n",
    "                    'right': right,\n",
    "                    'top': top,\n",
    "                    'label': 0,  # Negative sample\n",
    "                    'static_vars': ','.join(static_vars),\n",
    "                    'dynamic_vars': ','.join(dynamic_vars),\n",
    "                    'window_days': window_days,\n",
    "                }\n",
    "                if landcover_path:\n",
    "                    manifest_row['landcover_path'] = str(landcover_path)\n",
    "                    manifest_row['landcover_classes'] = num_landcover_classes\n",
    "                \n",
    "                manifest_rows.append(manifest_row)\n",
    "        \n",
    "        print(f\"  ✓ Year {year}: Generated {year_samples} negative samples (attempts: {attempts})\")\n",
    "\n",
    "    # Save manifest\n",
    "    manifest = pd.DataFrame(manifest_rows)\n",
    "    manifest_path = out_root / \"negative\" / \"negative_patches_manifest.csv\"\n",
    "    manifest.to_csv(manifest_path, index=False)\n",
    "\n",
    "    print(f\"\\n✅ Negative Dataset Complete!\")\n",
    "    print(f\"  Total patches: {len(manifest)}\")\n",
    "    print(f\"  Static shape: ({len(static_vars)}, {tile_size}, {tile_size})\")\n",
    "    print(f\"  Dynamic shape: ({window_days}, {len(dynamic_vars)}, {tile_size}, {tile_size})\")\n",
    "    if aligned_landcover:\n",
    "        print(f\"  Landcover shape: ({num_landcover_classes}, {tile_size}, {tile_size}) [one-hot encoded]\")\n",
    "    print(f\"  Manifest: {manifest_path}\")\n",
    "    print(f\"  Output directory: {out_root / 'negative'}\")\n",
    "\n",
    "    return manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b10a4af-fcfb-4d32-9398-89666299cb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ALIGNED FIRE DATASET BUILDER WITH LANDCOVER\n",
      "======================================================================\n",
      "\n",
      "[Step 0: Discovering Dynamic Rasters]\n",
      "  relative_humidity: found 3656 files\n",
      "  total_precipitation: found 3653 files\n",
      "  dew_point_temperature: found 3653 files\n",
      "  wind_speed: found 3653 files\n",
      "  temperature: found 4659 files\n",
      "  smi: found 3512 files\n",
      "\n",
      "✓ Total dynamic files found: 22786\n",
      "\n",
      "[Step 1: Creating Reference Grid]\n",
      "  ✓ Using existing reference grid: Dataset/Stataic Data/rasters_COP90/elevation_fixed_3310_1000m_clipped.tif\n",
      "\n",
      "[Step 2: Aligning Rasters]\n",
      "⚠️  This may take a while with ~3,650 dynamic files (10 years × 365 days)\n",
      "    Set skip_existing_aligned=True to skip already processed files\n",
      "\n",
      "[Aligning Static Rasters]\n",
      "  ✓ Using existing: elevation_aligned.tif\n",
      "  Computing statistics for elevation...\n",
      "    Min: -85.8572, Max: 4248.8701\n",
      "  ✓ Using existing: slope_aligned.tif\n",
      "  Computing statistics for slope...\n",
      "    Min: 0.0000, Max: 66.0898\n",
      "  ✓ Using existing: population_aligned.tif\n",
      "  Computing statistics for population...\n",
      "    Min: 0.0000, Max: 883.9456\n",
      "  ✓ Using existing: water_proximity_aligned.tif\n",
      "  Computing statistics for water_proximity...\n",
      "    Min: 0.0000, Max: 45700.9844\n",
      "  ✓ Using existing: road_proximity_aligned.tif\n",
      "  Computing statistics for road_proximity...\n",
      "    Min: 0.0000, Max: 21100.0000\n",
      "\n",
      "[Aligning Landcover Raster]\n",
      "  ✓ Using existing: landcover_aligned.tif\n",
      "\n",
      "[Aligning Dynamic Rasters]\n",
      "\n",
      "  Processing relative_humidity: 3656 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Aligning relative_humidity: 100%|█████████| 3656/3656 [00:59<00:00, 61.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Statistics for relative_humidity:\n",
      "    Min: 1.0000, Max: 100.0000\n",
      "\n",
      "  Processing total_precipitation: 3653 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Aligning total_precipitation: 100%|███████| 3653/3653 [00:44<00:00, 82.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Statistics for total_precipitation:\n",
      "    Min: 0.0000, Max: 490.9400\n",
      "\n",
      "  Processing dew_point_temperature: 3653 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Aligning dew_point_temperature: 100%|█████| 3653/3653 [00:49<00:00, 73.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Statistics for dew_point_temperature:\n",
      "    Min: 245.4046, Max: 302.0115\n",
      "\n",
      "  Processing wind_speed: 3653 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Aligning wind_speed: 100%|████████████████| 3653/3653 [00:47<00:00, 76.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Statistics for wind_speed:\n",
      "    Min: 0.1934, Max: 13.3605\n",
      "\n",
      "  Processing temperature: 4659 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Aligning temperature: 100%|███████████████| 4659/4659 [01:03<00:00, 73.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Statistics for temperature:\n",
      "    Min: 255.2278, Max: 324.4470\n",
      "\n",
      "  Processing smi: 3512 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Aligning smi: 100%|███████████████████████| 3512/3512 [00:41<00:00, 84.92it/s]\n",
      "/tmp/ipykernel_2282/1859636100.py:392: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  ca_union = ca.unary_union\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Statistics for smi:\n",
      "    Min: 0.0000, Max: 15.0000\n",
      "\n",
      "[Saving Statistics]\n",
      "  ✓ Statistics saved to: output_aligned_patches/statistics.json\n",
      "\n",
      "✓ Alignment complete\n",
      "  Static: 5 variables\n",
      "  Dynamic: 22786 files across 6 variables\n",
      "  Landcover: ✓ Aligned\n",
      "\n",
      "[Step 3: Extracting Aligned Fire Patches]\n",
      "  This will create .npy files for patches that overlap with fires\n",
      "\n",
      "[Reference Grid] 915x1056 px\n",
      "\n",
      "[Loading Fire Dataset]\n",
      "  ✓ Found 4032 fire polygons across 1671 unique dates\n",
      "\n",
      "[Indexing Dynamic Rasters]\n",
      "  relative_humidity: 3653 dates\n",
      "  total_precipitation: 3653 dates\n",
      "  dew_point_temperature: 3653 dates\n",
      "  wind_speed: 3653 dates\n",
      "  temperature: 4659 dates\n",
      "  smi: 3512 dates\n",
      "  ✓ 3391 dates available across all dynamic variables\n",
      "\n",
      "[Extracting Patches]\n",
      "  Static variables: ['elevation', 'slope', 'population', 'water_proximity', 'road_proximity']\n",
      "  Dynamic variables: ['relative_humidity', 'total_precipitation', 'dew_point_temperature', 'wind_speed', 'temperature', 'smi']\n",
      "  Landcover classes: 10\n",
      "  Window days: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dates: 100%|███████████████████| 3391/3391 [1:25:34<00:00,  1.51s/it]\n",
      "/tmp/ipykernel_2282/1859636100.py:773: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  ca_union = ca.unary_union\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Complete!\n",
      "  Total patches: 2841\n",
      "  Static shape: (5, 4, 4)\n",
      "  Dynamic shape: (10, 6, 4, 4)\n",
      "  Landcover shape: (10, 4, 4) [one-hot encoded]\n",
      "  Manifest: output_aligned_patches/aligned_patches_manifest.csv\n",
      "  Static TIFs: output_aligned_patches/static_tifs\n",
      "  Dynamic TIFs (first 2 days): output_aligned_patches/dynamic_tifs\n",
      "  Landcover TIFs: output_aligned_patches/landcover_tifs\n",
      "\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "GENERATING NEGATIVE DATASET\n",
      "======================================================================\n",
      "\n",
      "[Step 4: Extracting Negative Fire Patches]\n",
      "  This will create .npy files for patches with NO fire in 10-day window\n",
      "  Sampling 3000 patches per year (2015-2024)\n",
      "\n",
      "[Reference Grid] 915x1056 px\n",
      "\n",
      "[Loading Fire Dataset]\n",
      "  ✓ Found 4032 fire polygons across 1671 unique dates\n",
      "\n",
      "[Indexing Dynamic Rasters]\n",
      "  relative_humidity: 3653 dates\n",
      "  total_precipitation: 3653 dates\n",
      "  dew_point_temperature: 3653 dates\n",
      "  wind_speed: 3653 dates\n",
      "  temperature: 4659 dates\n",
      "  smi: 3512 dates\n",
      "  ✓ 3391 dates available across all dynamic variables\n",
      "\n",
      "[Extracting NEGATIVE Patches]\n",
      "  Static variables: ['elevation', 'slope', 'population', 'water_proximity', 'road_proximity']\n",
      "  Dynamic variables: ['relative_humidity', 'total_precipitation', 'dew_point_temperature', 'wind_speed', 'temperature', 'smi']\n",
      "  Landcover classes: 10\n",
      "  Window days: 10\n",
      "  Samples per year: 1000\n",
      "  Year range: 2015-2024\n",
      "\n",
      "[Pre-computing valid tile positions]\n",
      "  ✓ Found 24971 valid tile positions inside California\n",
      "\n",
      "[Processing Year 2015]\n",
      "  Valid dates with 10-day window: 247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Sampling year 2015: 100%|█████████████████| 1000/1000 [03:07<00:00,  5.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Year 2015: Generated 1000 negative samples (attempts: 1017)\n",
      "\n",
      "[Processing Year 2016]\n",
      "  Valid dates with 10-day window: 315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Sampling year 2016: 100%|█████████████████| 1000/1000 [03:27<00:00,  4.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Year 2016: Generated 1000 negative samples (attempts: 1009)\n",
      "\n",
      "[Processing Year 2017]\n",
      "  Valid dates with 10-day window: 319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Sampling year 2017: 100%|█████████████████| 1000/1000 [03:13<00:00,  5.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Year 2017: Generated 1000 negative samples (attempts: 1015)\n",
      "\n",
      "[Processing Year 2018]\n",
      "  Valid dates with 10-day window: 365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Sampling year 2018: 100%|█████████████████| 1000/1000 [03:05<00:00,  5.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Year 2018: Generated 1000 negative samples (attempts: 1010)\n",
      "\n",
      "[Processing Year 2019]\n",
      "  Valid dates with 10-day window: 323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Sampling year 2019: 100%|█████████████████| 1000/1000 [02:57<00:00,  5.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Year 2019: Generated 1000 negative samples (attempts: 1008)\n",
      "\n",
      "[Processing Year 2020]\n",
      "  Valid dates with 10-day window: 366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Sampling year 2020: 100%|█████████████████| 1000/1000 [03:32<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Year 2020: Generated 1000 negative samples (attempts: 1010)\n",
      "\n",
      "[Processing Year 2021]\n",
      "  Valid dates with 10-day window: 365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Sampling year 2021: 100%|█████████████████| 1000/1000 [03:48<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Year 2021: Generated 1000 negative samples (attempts: 1011)\n",
      "\n",
      "[Processing Year 2022]\n",
      "  Valid dates with 10-day window: 260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Sampling year 2022: 100%|█████████████████| 1000/1000 [02:56<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Year 2022: Generated 1000 negative samples (attempts: 1010)\n",
      "\n",
      "[Processing Year 2023]\n",
      "  Valid dates with 10-day window: 365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Sampling year 2023: 100%|█████████████████| 1000/1000 [03:01<00:00,  5.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Year 2023: Generated 1000 negative samples (attempts: 1018)\n",
      "\n",
      "[Processing Year 2024]\n",
      "  Valid dates with 10-day window: 366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Sampling year 2024: 100%|█████████████████| 1000/1000 [03:14<00:00,  5.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Year 2024: Generated 1000 negative samples (attempts: 1009)\n",
      "\n",
      "✅ Negative Dataset Complete!\n",
      "  Total patches: 10000\n",
      "  Static shape: (5, 4, 4)\n",
      "  Dynamic shape: (10, 6, 4, 4)\n",
      "  Landcover shape: (10, 4, 4) [one-hot encoded]\n",
      "  Manifest: output_aligned_patches/negative/negative_patches_manifest.csv\n",
      "  Output directory: output_aligned_patches/negative\n",
      "\n",
      "======================================================================\n",
      "📊 NEGATIVE DATASET SUMMARY\n",
      "======================================================================\n",
      "Total negative patches extracted: 10000\n",
      "\n",
      "Static patch shape: (5, 4, 4)\n",
      "Dynamic patch shape: (10, 6, 4, 4)\n",
      "Landcover patch shape: (10, 4, 4) [one-hot encoded]\n",
      "\n",
      "Output directory: output_aligned_patches/negative\n",
      "Manifest file: output_aligned_patches/negative/negative_patches_manifest.csv\n",
      "\n",
      "📝 First few patches:\n",
      "   patch_id                                        static_path  \\\n",
      "0         1  output_aligned_patches/negative/20150717_928_8...   \n",
      "1         2  output_aligned_patches/negative/20151120_20_24...   \n",
      "2         3  output_aligned_patches/negative/20150914_540_3...   \n",
      "3         4  output_aligned_patches/negative/20150920_532_5...   \n",
      "4         5  output_aligned_patches/negative/20150710_384_4...   \n",
      "5         6  output_aligned_patches/negative/20150701_644_6...   \n",
      "6         7  output_aligned_patches/negative/20151125_584_2...   \n",
      "7         8  output_aligned_patches/negative/20151203_444_4...   \n",
      "8         9  output_aligned_patches/negative/20151210_644_3...   \n",
      "9        10  output_aligned_patches/negative/20150422_744_7...   \n",
      "\n",
      "                                        dynamic_path sample_date  year  \\\n",
      "0  output_aligned_patches/negative/20150717_928_8...  2015-07-17  2015   \n",
      "1  output_aligned_patches/negative/20151120_20_24...  2015-11-20  2015   \n",
      "2  output_aligned_patches/negative/20150914_540_3...  2015-09-14  2015   \n",
      "3  output_aligned_patches/negative/20150920_532_5...  2015-09-20  2015   \n",
      "4  output_aligned_patches/negative/20150710_384_4...  2015-07-10  2015   \n",
      "5  output_aligned_patches/negative/20150701_644_6...  2015-07-01  2015   \n",
      "6  output_aligned_patches/negative/20151125_584_2...  2015-11-25  2015   \n",
      "7  output_aligned_patches/negative/20151203_444_4...  2015-12-03  2015   \n",
      "8  output_aligned_patches/negative/20151210_644_3...  2015-12-10  2015   \n",
      "9  output_aligned_patches/negative/20150422_744_7...  2015-04-22  2015   \n",
      "\n",
      "   row_off  col_off           left         bottom          right  \\\n",
      "0      928      868  493572.789156 -481505.225725  497572.789156   \n",
      "1       20      240 -134427.210844  426494.774275 -130427.210844   \n",
      "2      540      324  -50427.210844  -93505.225725  -46427.210844   \n",
      "3      532      532  157572.789156  -85505.225725  161572.789156   \n",
      "4      384      408   33572.789156   62494.774275   37572.789156   \n",
      "5      644      676  301572.789156 -197505.225725  305572.789156   \n",
      "6      584      256 -118427.210844 -137505.225725 -114427.210844   \n",
      "7      444      464   89572.789156    2494.774275   93572.789156   \n",
      "8      644      312  -62427.210844 -197505.225725  -58427.210844   \n",
      "9      744      712  337572.789156 -297505.225725  341572.789156   \n",
      "\n",
      "             top  label                                        static_vars  \\\n",
      "0 -477505.225725      0  elevation,slope,population,water_proximity,roa...   \n",
      "1  430494.774275      0  elevation,slope,population,water_proximity,roa...   \n",
      "2  -89505.225725      0  elevation,slope,population,water_proximity,roa...   \n",
      "3  -81505.225725      0  elevation,slope,population,water_proximity,roa...   \n",
      "4   66494.774275      0  elevation,slope,population,water_proximity,roa...   \n",
      "5 -193505.225725      0  elevation,slope,population,water_proximity,roa...   \n",
      "6 -133505.225725      0  elevation,slope,population,water_proximity,roa...   \n",
      "7    6494.774275      0  elevation,slope,population,water_proximity,roa...   \n",
      "8 -193505.225725      0  elevation,slope,population,water_proximity,roa...   \n",
      "9 -293505.225725      0  elevation,slope,population,water_proximity,roa...   \n",
      "\n",
      "                                        dynamic_vars  window_days  \\\n",
      "0  relative_humidity,total_precipitation,dew_poin...           10   \n",
      "1  relative_humidity,total_precipitation,dew_poin...           10   \n",
      "2  relative_humidity,total_precipitation,dew_poin...           10   \n",
      "3  relative_humidity,total_precipitation,dew_poin...           10   \n",
      "4  relative_humidity,total_precipitation,dew_poin...           10   \n",
      "5  relative_humidity,total_precipitation,dew_poin...           10   \n",
      "6  relative_humidity,total_precipitation,dew_poin...           10   \n",
      "7  relative_humidity,total_precipitation,dew_poin...           10   \n",
      "8  relative_humidity,total_precipitation,dew_poin...           10   \n",
      "9  relative_humidity,total_precipitation,dew_poin...           10   \n",
      "\n",
      "                                      landcover_path  landcover_classes  \n",
      "0  output_aligned_patches/negative/20150717_928_8...                 10  \n",
      "1  output_aligned_patches/negative/20151120_20_24...                 10  \n",
      "2  output_aligned_patches/negative/20150914_540_3...                 10  \n",
      "3  output_aligned_patches/negative/20150920_532_5...                 10  \n",
      "4  output_aligned_patches/negative/20150710_384_4...                 10  \n",
      "5  output_aligned_patches/negative/20150701_644_6...                 10  \n",
      "6  output_aligned_patches/negative/20151125_584_2...                 10  \n",
      "7  output_aligned_patches/negative/20151203_444_4...                 10  \n",
      "8  output_aligned_patches/negative/20151210_644_3...                 10  \n",
      "9  output_aligned_patches/negative/20150422_744_7...                 10  \n",
      "\n",
      "📅 Samples per year:\n",
      "year\n",
      "2015    1000\n",
      "2016    1000\n",
      "2017    1000\n",
      "2018    1000\n",
      "2019    1000\n",
      "2020    1000\n",
      "2021    1000\n",
      "2022    1000\n",
      "2023    1000\n",
      "2024    1000\n",
      "dtype: int64\n",
      "\n",
      "======================================================================\n",
      "📊 SUMMARY\n",
      "======================================================================\n",
      "Total fire patches extracted: 2841\n",
      "\n",
      "Static patch shape: (5, 4, 4)\n",
      "Dynamic patch shape: (10, 6, 4, 4)\n",
      "Landcover patch shape: (10, 4, 4) [one-hot encoded]\n",
      "\n",
      "Output directory: output_aligned_patches\n",
      "Manifest file: output_aligned_patches/aligned_patches_manifest.csv\n",
      "\n",
      "📝 First few patches:\n",
      "   patch_id                                        static_path  \\\n",
      "0         1  output_aligned_patches/positive/20150617_868_6...   \n",
      "1         2  output_aligned_patches/positive/20150617_872_6...   \n",
      "2         3  output_aligned_patches/positive/20150617_872_6...   \n",
      "3         4  output_aligned_patches/positive/20150617_876_6...   \n",
      "4         5  output_aligned_patches/positive/20150617_876_6...   \n",
      "5         6  output_aligned_patches/positive/20150617_876_6...   \n",
      "6         7  output_aligned_patches/positive/20150619_376_3...   \n",
      "7         8  output_aligned_patches/positive/20150619_376_4...   \n",
      "8         9  output_aligned_patches/positive/20150619_380_3...   \n",
      "9        10  output_aligned_patches/positive/20150619_380_4...   \n",
      "\n",
      "                                        dynamic_path   fire_date  row_off  \\\n",
      "0  output_aligned_patches/positive/20150617_868_6...  2015-06-17      868   \n",
      "1  output_aligned_patches/positive/20150617_872_6...  2015-06-17      872   \n",
      "2  output_aligned_patches/positive/20150617_872_6...  2015-06-17      872   \n",
      "3  output_aligned_patches/positive/20150617_876_6...  2015-06-17      876   \n",
      "4  output_aligned_patches/positive/20150617_876_6...  2015-06-17      876   \n",
      "5  output_aligned_patches/positive/20150617_876_6...  2015-06-17      876   \n",
      "6  output_aligned_patches/positive/20150619_376_3...  2015-06-19      376   \n",
      "7  output_aligned_patches/positive/20150619_376_4...  2015-06-19      376   \n",
      "8  output_aligned_patches/positive/20150619_380_3...  2015-06-19      380   \n",
      "9  output_aligned_patches/positive/20150619_380_4...  2015-06-19      380   \n",
      "\n",
      "   col_off           left         bottom          right            top  \\\n",
      "0      680  305572.789156 -421505.225725  309572.789156 -417505.225725   \n",
      "1      664  289572.789156 -425505.225725  293572.789156 -421505.225725   \n",
      "2      680  305572.789156 -425505.225725  309572.789156 -421505.225725   \n",
      "3      664  289572.789156 -429505.225725  293572.789156 -425505.225725   \n",
      "4      668  293572.789156 -429505.225725  297572.789156 -425505.225725   \n",
      "5      672  297572.789156 -429505.225725  301572.789156 -425505.225725   \n",
      "6      396   21572.789156   70494.774275   25572.789156   74494.774275   \n",
      "7      400   25572.789156   70494.774275   29572.789156   74494.774275   \n",
      "8      396   21572.789156   66494.774275   25572.789156   70494.774275   \n",
      "9      400   25572.789156   66494.774275   29572.789156   70494.774275   \n",
      "\n",
      "   overlap_frac                                        static_vars  \\\n",
      "0      0.852529  elevation,slope,population,water_proximity,roa...   \n",
      "1      0.651433  elevation,slope,population,water_proximity,roa...   \n",
      "2      0.767612  elevation,slope,population,water_proximity,roa...   \n",
      "3      0.683280  elevation,slope,population,water_proximity,roa...   \n",
      "4      0.987896  elevation,slope,population,water_proximity,roa...   \n",
      "5      0.922052  elevation,slope,population,water_proximity,roa...   \n",
      "6      0.773129  elevation,slope,population,water_proximity,roa...   \n",
      "7      0.714191  elevation,slope,population,water_proximity,roa...   \n",
      "8      0.847173  elevation,slope,population,water_proximity,roa...   \n",
      "9      0.801302  elevation,slope,population,water_proximity,roa...   \n",
      "\n",
      "                                        dynamic_vars  window_days  \\\n",
      "0  relative_humidity,total_precipitation,dew_poin...           10   \n",
      "1  relative_humidity,total_precipitation,dew_poin...           10   \n",
      "2  relative_humidity,total_precipitation,dew_poin...           10   \n",
      "3  relative_humidity,total_precipitation,dew_poin...           10   \n",
      "4  relative_humidity,total_precipitation,dew_poin...           10   \n",
      "5  relative_humidity,total_precipitation,dew_poin...           10   \n",
      "6  relative_humidity,total_precipitation,dew_poin...           10   \n",
      "7  relative_humidity,total_precipitation,dew_poin...           10   \n",
      "8  relative_humidity,total_precipitation,dew_poin...           10   \n",
      "9  relative_humidity,total_precipitation,dew_poin...           10   \n",
      "\n",
      "                                      landcover_path  landcover_classes  \n",
      "0  output_aligned_patches/positive/20150617_868_6...                 10  \n",
      "1  output_aligned_patches/positive/20150617_872_6...                 10  \n",
      "2  output_aligned_patches/positive/20150617_872_6...                 10  \n",
      "3  output_aligned_patches/positive/20150617_876_6...                 10  \n",
      "4  output_aligned_patches/positive/20150617_876_6...                 10  \n",
      "5  output_aligned_patches/positive/20150617_876_6...                 10  \n",
      "6  output_aligned_patches/positive/20150619_376_3...                 10  \n",
      "7  output_aligned_patches/positive/20150619_376_4...                 10  \n",
      "8  output_aligned_patches/positive/20150619_380_3...                 10  \n",
      "9  output_aligned_patches/positive/20150619_380_4...                 10  \n",
      "\n",
      "📅 Date range:\n",
      "  Earliest: 2015-06-17\n",
      "  Latest: 2024-12-09\n",
      "\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'ca_boundary': 'Dataset/Stataic Data/CA_State.gpkg',\n",
    "    'fire_dataset': 'Dataset/Stataic Data/past_fire_2014_2024.gpkg',\n",
    "    'fire_date_field': 'ALARM_DATE',\n",
    "    'static_rasters': {\n",
    "        'elevation': 'Dataset/Stataic Data/rasters_COP90/output_hh.tif',\n",
    "        'slope': 'Dataset/Stataic Data/viz/Slope.tif',\n",
    "        'population': 'Dataset/Stataic Data/Population/mosaic_masked.tif',\n",
    "        'water_proximity': 'Dataset/Stataic Data/Waterway/ca_water_distance.tif',\n",
    "        'road_proximity': 'Dataset/Stataic Data/roadways/ca_road_distance_snapped.tif',\n",
    "    },\n",
    "    'landcover_raster': 'Dataset/Stataic Data/LandCover/land_cover_cal_reclass.tif', \n",
    "    \n",
    "    'dynamic_folders': {\n",
    "        'relative_humidity': 'Dataset/Dynamic Data/relative_humidity',\n",
    "        'total_precipitation': 'Dataset/Dynamic Data/Precipitation',\n",
    "        'dew_point_temperature':'Dataset/Dynamic Data/DewPointTemperature',\n",
    "        'wind_speed':'Dataset/Dynamic Data/Wind_Speed',\n",
    "        'temperature':'Dataset/Dynamic Data/Temperature',\n",
    "        'smi':'Dataset/Dynamic Data/SMI'\n",
    "    },\n",
    "    'reference_grid_path': 'Dataset/Stataic Data/rasters_COP90/elevation_fixed_3310_1000m_clipped.tif',\n",
    "    'out_root': 'output_aligned_patches',\n",
    "    'tile_size': 4,\n",
    "    'window_days': 10,  \n",
    "    'dst_crs': 'EPSG:3310',\n",
    "    'dst_res': 1000.0,  \n",
    "    'skip_existing_aligned': True,  \n",
    "    'min_pos_frac': 0.50,  \n",
    "    'require_all_valid': True,\n",
    "    'num_landcover_classes': 10,\n",
    "    'write_individual_landcover_tifs': True,\n",
    "}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ALIGNED FIRE DATASET BUILDER WITH LANDCOVER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n[Step 0: Discovering Dynamic Rasters]\")\n",
    "dynamic_rasters = discover_dynamic_rasters(\n",
    "    dynamic_folders=config['dynamic_folders'],\n",
    "    pattern=\"*.tif\"\n",
    ")\n",
    "print(f\"\\n✓ Total dynamic files found: {sum(len(files) for files in dynamic_rasters.values())}\")\n",
    "\n",
    "print(\"\\n[Step 1: Creating Reference Grid]\")\n",
    "if os.path.exists(config['reference_grid_path']):\n",
    "    print(f\"  ✓ Using existing reference grid: {config['reference_grid_path']}\")\n",
    "    ref_grid = config['reference_grid_path']\n",
    "else:\n",
    "    ref_grid = create_reference_grid(\n",
    "        ca_boundary=config['ca_boundary'],\n",
    "        out_path=config['reference_grid_path'],\n",
    "        dst_crs=config['dst_crs'],\n",
    "        dst_res=config['dst_res']\n",
    "    )\n",
    "\n",
    "print(\"\\n[Step 2: Aligning Rasters]\")\n",
    "print(\"⚠️  This may take a while with ~3,650 dynamic files (10 years × 365 days)\")\n",
    "print(\"    Set skip_existing_aligned=True to skip already processed files\")\n",
    "aligned_static, aligned_dynamic, aligned_landcover = align_all_rasters(\n",
    "    static_rasters=config['static_rasters'],\n",
    "    dynamic_rasters=dynamic_rasters,\n",
    "    landcover_raster=config.get('landcover_raster'),  # NEW\n",
    "    reference_grid=ref_grid,\n",
    "    ca_boundary=config['ca_boundary'],\n",
    "    out_root=config['out_root'],\n",
    "    skip_existing=config['skip_existing_aligned']\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Alignment complete\")\n",
    "print(f\"  Static: {len(aligned_static)} variables\")\n",
    "print(f\"  Dynamic: {sum(len(files) for files in aligned_dynamic.values())} files across {len(aligned_dynamic)} variables\")\n",
    "if aligned_landcover:\n",
    "    print(f\"  Landcover: ✓ Aligned\")\n",
    "\n",
    "print(\"\\n[Step 3: Extracting Aligned Fire Patches]\")\n",
    "print(\"  This will create .npy files for patches that overlap with fires\")\n",
    "manifest = build_aligned_fire_dataset(\n",
    "    aligned_static=aligned_static,\n",
    "    aligned_dynamic=aligned_dynamic,\n",
    "    aligned_landcover=aligned_landcover,  # NEW\n",
    "    fire_dataset=config['fire_dataset'],\n",
    "    fire_date_field=config['fire_date_field'],\n",
    "    ca_boundary=config['ca_boundary'],\n",
    "    out_root=config['out_root'],\n",
    "    tile_size=config['tile_size'],\n",
    "    window_days=config['window_days'],\n",
    "    min_pos_frac=config['min_pos_frac'],\n",
    "    require_all_valid=config['require_all_valid'],\n",
    "    num_landcover_classes=config.get('num_landcover_classes', 10),  # NEW\n",
    "    write_individual_landcover_tifs=config.get('write_individual_landcover_tifs', True),  # NEW\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# STEP 4: Generate Negative Dataset (No Fire Samples)\n",
    "# =========================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GENERATING NEGATIVE DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n[Step 4: Extracting Negative Fire Patches]\")\n",
    "print(\"  This will create .npy files for patches with NO fire in 10-day window\")\n",
    "print(\"  Sampling 3000 patches per year (2015-2024)\")\n",
    "\n",
    "negative_manifest = build_negative_fire_dataset(\n",
    "    aligned_static=aligned_static,\n",
    "    aligned_dynamic=aligned_dynamic,\n",
    "    aligned_landcover=aligned_landcover,\n",
    "    fire_dataset=config['fire_dataset'],\n",
    "    fire_date_field=config['fire_date_field'],\n",
    "    ca_boundary=config['ca_boundary'],\n",
    "    out_root=config['out_root'],\n",
    "    tile_size=config['tile_size'],\n",
    "    window_days=config['window_days'],\n",
    "    samples_per_year=1000,\n",
    "    min_valid_frac=config.get('min_valid_frac', 1),\n",
    "    require_all_valid=config['require_all_valid'],\n",
    "    num_landcover_classes=config.get('num_landcover_classes', 10),\n",
    "    start_year=2015,\n",
    "    end_year=2024,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"📊 NEGATIVE DATASET SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total negative patches extracted: {len(negative_manifest)}\")\n",
    "print(f\"\\nStatic patch shape: ({len(aligned_static)}, {config['tile_size']}, {config['tile_size']})\")\n",
    "print(f\"Dynamic patch shape: ({config['window_days']}, {len(aligned_dynamic)}, {config['tile_size']}, {config['tile_size']})\")\n",
    "if aligned_landcover:\n",
    "    print(f\"Landcover patch shape: ({config['num_landcover_classes']}, {config['tile_size']}, {config['tile_size']}) [one-hot encoded]\")\n",
    "\n",
    "print(f\"\\nOutput directory: {config['out_root']}/negative\")\n",
    "print(f\"Manifest file: {config['out_root']}/negative/negative_patches_manifest.csv\")\n",
    "\n",
    "if len(negative_manifest) > 0:\n",
    "    print(f\"\\n📝 First few patches:\")\n",
    "    print(negative_manifest.head(10))\n",
    "    \n",
    "    print(f\"\\n📅 Samples per year:\")\n",
    "    print(negative_manifest.groupby('year').size())\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"📊 SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total fire patches extracted: {len(manifest)}\")\n",
    "print(f\"\\nStatic patch shape: ({len(aligned_static)}, {config['tile_size']}, {config['tile_size']})\")\n",
    "print(f\"Dynamic patch shape: ({config['window_days']}, {len(aligned_dynamic)}, {config['tile_size']}, {config['tile_size']})\")\n",
    "if aligned_landcover:\n",
    "    print(f\"Landcover patch shape: ({config['num_landcover_classes']}, {config['tile_size']}, {config['tile_size']}) [one-hot encoded]\")\n",
    "print(f\"\\nOutput directory: {config['out_root']}\")\n",
    "print(f\"Manifest file: {config['out_root']}/aligned_patches_manifest.csv\")\n",
    "\n",
    "if len(manifest) > 0:\n",
    "    print(f\"\\n📝 First few patches:\")\n",
    "    print(manifest.head(10))\n",
    "    print(f\"\\n📅 Date range:\")\n",
    "    print(f\"  Earliest: {manifest['fire_date'].min()}\")\n",
    "    print(f\"  Latest: {manifest['fire_date'].max()}\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c14d7a-504b-4582-8bb5-da2a338e838e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
